{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21364c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import isnan, when, count, col,mean\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, LongType,\n",
    "    StringType, FloatType, TimestampType\n",
    ")\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JapanTradeStats\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"category_id\", StringType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"user_id\", LongType(), True),\n",
    "    StructField(\"user_session\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load CSV with schema\n",
    "df = spark.read.csv(\n",
    "    \"file:///home/eng-mohammed/Desktop/Data.csv\",\n",
    "    header=True,\n",
    "    schema=schema\n",
    ")\n",
    "data =df\n",
    "data.cache()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b73380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------part2--------------------------------------\n",
    "# understant data\n",
    "print(\"Total data is : \",data.count())\n",
    "print(\"describe data : \",data.describe())\n",
    "# data.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = data.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in data.columns\n",
    "])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67443355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, mean \n",
    "df = data.withColumn(\"product_id\", when(col(\"product_id\")==\"NULL\",None).otherwise(col(\"product_id\"))) \\\n",
    "         .withColumn(\"brand\", when(col(\"brand\")==\"NULL\",None).otherwise(col(\"brand\"))) \\\n",
    "         .withColumn(\"price\", when(col(\"price\")==\"NULL\",None).otherwise(col(\"price\")))\\\n",
    "         .withColumn(\"category_code\",when(col(\"category_code\")==\"NULL\",None).otherwise(col(\"category_code\")))\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Filter invalid product_id\n",
    "df = df.filter(col(\"product_id\").isNotNull())\n",
    "\n",
    "# Fill missing prices with mean\n",
    "mean_price = df.select(mean(\"price\")).collect()[0][0]\n",
    "\n",
    "df_clean = df.withColumn(\n",
    "    \"price\",\n",
    "    when(col(\"price\").isNull(), mean_price).otherwise(col(\"price\"))\n",
    ")\n",
    "\n",
    "df_clean.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = (df_clean\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
    "    .withColumn(\"event_month\",F.month(\"event_time\"))\n",
    "    .withColumn(\"event_hour\", F.hour(\"event_time\"))\n",
    "    .withColumn(\"category_main\",\n",
    "                F.when(F.col(\"category_code\").isNotNull(),\n",
    "                       F.split(\"category_code\", \"\\\\.\")[0])\n",
    "                .otherwise(None))\n",
    ")\n",
    "df_fe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dist = df_fe.groupBy(\"event_type\").count()\n",
    "events_dist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31c278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_viewed_products = (df_fe\n",
    "    .filter(F.col(\"event_type\") == \"view\")\n",
    "    .groupBy([\"product_id\",\"category_main\"])\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "top_viewed_products.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8144666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1️⃣ Clean & cast price to double\n",
    "df_fe = df_fe.withColumn(\n",
    "    \"price\",\n",
    "    F.regexp_replace(\"price\", \",\", \"\")  # remove commas if any\n",
    "     .cast(DoubleType())\n",
    ")\n",
    "\n",
    "# 2️⃣ Optional: remove duplicates based on user_session + product_id + event_time\n",
    "df_fe = df_fe.dropDuplicates([\"user_session\", \"product_id\", \"event_time\"])\n",
    "\n",
    "# 3️⃣ Filter for valid purchases\n",
    "df_purchase = df_fe.filter(\n",
    "    (F.col(\"event_type\") == \"purchase\") &\n",
    "    (F.col(\"brand\").isNotNull()) &\n",
    "    (F.col(\"price\").isNotNull()) &\n",
    "    (F.col(\"price\") > 0)\n",
    ")\n",
    "\n",
    "# 4️⃣ Aggregate revenue by brand\n",
    "brand_revenue = (\n",
    "    df_purchase\n",
    "    .groupBy(\"brand\")\n",
    "    .agg(F.round(F.sum(\"price\"),2).alias(\"total_revenue\"))\n",
    "    .orderBy(F.desc(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "# 5️⃣ Show top 10 brands by revenue\n",
    "brand_revenue.show(10, truncate=False)\n",
    "\n",
    "# 6️⃣ Check schema (optional)\n",
    "brand_revenue.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hourly_traffic = (\n",
    "    df_fe\n",
    "    .groupBy(\"event_hour\")   # only by hour\n",
    "    .count()                 # count all events in that hour\n",
    "    .orderBy(\"event_hour\")   # order by hour\n",
    ")\n",
    "\n",
    "hourly_traffic.show(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_revenue = (\n",
    "    df_fe\n",
    "    .filter(F.col(\"event_type\") == \"purchase\")\n",
    "    .groupBy(\"event_hour\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_purchases\"),\n",
    "        F.round(F.sum(\"price\"),2).alias(\"total_revenue\")\n",
    "    )\n",
    "    .orderBy(\"event_hour\")\n",
    ")\n",
    "\n",
    "hourly_revenue.show(24)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
